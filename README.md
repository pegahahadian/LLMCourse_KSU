# Next Token Prediction with Transformers

This project implements and compares two neural network architectures for next-token prediction using the [WikiText-2](https://paperswithcode.com/dataset/wikitext-2) dataset:

1. **Baseline Feedforward Neural Network (MLP)**
2. **Transformer-based Model with Multi-head Self-Attention**

The goal is to evaluate how attention mechanisms enhance performance in sequence modeling tasks like language modeling.

---

## Project Structure

- `Next Token Prediction.ipynb` — Main Jupyter notebook with all implementation steps.
- `requirements.txt` — Python package dependencies.
- `README.md` — Project documentation.

---

##  Installation

Make sure Python ≥ 3.7 is installed. Then install the required packages:

```bash
pip install -r requirements.txt
