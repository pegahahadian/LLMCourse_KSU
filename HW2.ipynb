{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963dbf9d-101f-41a8-8617-45f636c000d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Inputs: tensor([[ 2015,  3793,  2013, 15536, 23615, 10288,  2102,  1011],\n",
      "        [ 3217,  9623,  2015,  3793,  2013, 15536, 23615, 10288],\n",
      "        [ 7170,  1998, 17463,  3217,  9623,  2015,  3793,  2013],\n",
      "        [ 9623,  2015,  3793,  2013, 15536, 23615, 10288,  2102],\n",
      "        [  101,  7170,  1998, 17463,  3217,  9623,  2015,  3793],\n",
      "        [17463,  3217,  9623,  2015,  3793,  2013, 15536, 23615],\n",
      "        [ 1998, 17463,  3217,  9623,  2015,  3793,  2013, 15536]])\n",
      "Sample Targets: tensor([  102,  1011, 23615,  1016, 15536,  2102, 10288])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, targets = batch\n",
    "    print(f\"Sample Inputs: {inputs}\")\n",
    "    print(f\"Sample Targets: {targets}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d338b4-76f3-4c9a-af6f-b3e0ec038442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, vocab_size=10000, seq_length=8):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = [token if token < vocab_size else 1 for token in tokens[:-1]]  # Replace OOV tokens with <UNK> (index 1)\n",
    "        self.labels = [token if token < vocab_size else 1 for token in tokens[1:]]  # Same for targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx + self.seq_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx + self.seq_length], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01703d2c-d62f-42a2-82ba-c14678f7fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token index after preprocessing: 23615\n"
     ]
    }
   ],
   "source": [
    "max_index = max(dataset.data)\n",
    "print(f\"Max token index after preprocessing: {max_index}\")  # Should be ≤ 9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a0f628-2f35-4e45-ac9b-cb7001509a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, vocab_size=10000, seq_length=8):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Replace tokens that exceed vocab_size with <UNK> token (index 1)\n",
    "        self.data = [token if token < vocab_size else 1 for token in tokens[:-1]]\n",
    "        self.labels = [token if token < vocab_size else 1 for token in tokens[1:]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx + self.seq_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx + self.seq_length], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb1a5a7-fde4-4a31-942e-7a81a640c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token index after preprocessing: 9623\n"
     ]
    }
   ],
   "source": [
    "tokens = load_wikitext2() \n",
    "dataset = TokenDataset(tokens, vocab_size=10000, seq_length=8)  # Apply fix\n",
    "\n",
    "max_token_index = max(dataset.data)\n",
    "print(f\"Max token index after preprocessing: {max_token_index}\")  # Should be ≤ 9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c93f7241-cc10-4c53-a647-2d950c2c4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.2122\n",
      "Epoch 2, Loss: 8.6697\n",
      "Epoch 3, Loss: 8.1509\n",
      "Epoch 4, Loss: 7.5102\n",
      "Epoch 5, Loss: 6.7088\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "baseline_model = BaselineModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=LR_BASELINE)\n",
    "\n",
    "train_model(baseline_model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "977615bd-bc69-48d7-a222-90f45cc12b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Perplexity: 819.59\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_loss = 6.7088  \n",
    "perplexity = np.exp(final_loss)\n",
    "print(f\"Baseline Model Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c26f15b-6b49-4bb3-8378-9e92c1146f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 100.00%\n",
      "Top-3 Accuracy: 100.00%\n",
      "Top-5 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct_top1, correct_top3, correct_top5, total = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs) \n",
    "            probs = torch.softmax(outputs, dim=-1)  \n",
    "            top_5_preds = torch.topk(probs, k=5, dim=-1).indices  \n",
    "            \n",
    "            correct_top1 += (top_5_preds[:, 0] == targets).sum().item()\n",
    "            correct_top3 += (targets.unsqueeze(1) == top_5_preds[:, :3]).sum().item()\n",
    "            correct_top5 += (targets.unsqueeze(1) == top_5_preds[:, :5]).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    print(f\"Top-1 Accuracy: {100 * correct_top1 / total:.2f}%\")\n",
    "    print(f\"Top-3 Accuracy: {100 * correct_top3 / total:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {100 * correct_top5 / total:.2f}%\")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "evaluate_model(baseline_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d992c7-e683-47e2-829f-e81c935cdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.1919\n",
      "Epoch 2, Loss: 9.1429\n",
      "Epoch 3, Loss: 8.9537\n",
      "Epoch 4, Loss: 8.7684\n",
      "Epoch 5, Loss: 8.6531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pahad\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Transformer Model\n",
    "transformer_model = TransformerModel(VOCAB_SIZE, EMBED_DIM, N_HEADS, FF_DIM, DROPOUT)\n",
    "optimizer_transformer = optim.Adam(transformer_model.parameters(), lr=LR_TRANSFORMER)\n",
    "\n",
    "train_model(transformer_model, train_loader, criterion, optimizer_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d83b1874-c003-4e46-ab67-cfd18fb49842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (2.5.0+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.10.10)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.9.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets) (1.15.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.5.3)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pahad\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.9/11.8 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.8 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.8 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 12.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 18.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets spacy numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5491a826-4519-410b-91d2-b1b9d6a4b1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cb1be47cb7471682e3e9d331400795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3871f53af5a9476facf4d33816c2ae2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2a46e2d0b4477393b3abe541e081a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d07e99ef40547f7a675bfb729bc5095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30878a5af2f94bd0bbd61d3c692c2c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8836bd9d604d218e2fae086944e417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully downloaded and loaded!\n",
      "Train dataset length: 10929707 characters\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# WikiText-2 \n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "train_text = dataset[\"train\"][\"text\"]\n",
    "valid_text = dataset[\"validation\"][\"text\"]\n",
    "test_text = dataset[\"test\"][\"text\"]\n",
    "\n",
    "train_text = \" \".join(train_text)\n",
    "valid_text = \" \".join(valid_text)\n",
    "test_text = \" \".join(test_text)\n",
    "\n",
    "print(\"Dataset successfully downloaded and loaded!\")\n",
    "print(f\"Train dataset length: {len(train_text)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eba1031b-fc9d-4904-9a93-255423341765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2303697 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in train set: 2303697\n",
      "Total tokens in validation set: 238658\n",
      "Total tokens in test set: 273180\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_tokens = tokenizer.encode(train_text, add_special_tokens=True)\n",
    "valid_tokens = tokenizer.encode(valid_text, add_special_tokens=True)\n",
    "test_tokens = tokenizer.encode(test_text, add_special_tokens=True)\n",
    "\n",
    "print(f\"Total tokens in train set: {len(train_tokens)}\")\n",
    "print(f\"Total tokens in validation set: {len(valid_tokens)}\")\n",
    "print(f\"Total tokens in test set: {len(test_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aefbd7a-91f8-499c-b962-04a215a87950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "VOCAB_SIZE = 10000  \n",
    "\n",
    "token_counts = Counter(train_tokens)\n",
    "\n",
    "most_common_tokens = token_counts.most_common(VOCAB_SIZE)\n",
    "\n",
    "token_to_index = {token: i for i, (token, _) in enumerate(most_common_tokens)}\n",
    "\n",
    "PAD_ID, UNK_ID, BOS_ID, EOS_ID = 0, 1, 2, 3\n",
    "\n",
    "train_tokens = [token_to_index.get(token, UNK_ID) for token in train_tokens]\n",
    "valid_tokens = [token_to_index.get(token, UNK_ID) for token in valid_tokens]\n",
    "test_tokens = [token_to_index.get(token, UNK_ID) for token in test_tokens]\n",
    "\n",
    "print(f\"Vocabulary size: {len(token_to_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06525c4d-7621-47b1-8a4a-da39330abfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 2303688 sequences\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEQ_LENGTH = 8 \n",
    "BATCH_SIZE = 64  \n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens, seq_length=SEQ_LENGTH):\n",
    "        self.seq_length = seq_length\n",
    "        self.data = tokens[:-1]  \n",
    "        self.labels = tokens[1:]  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx+self.seq_length], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "train_dataset = TokenDataset(train_tokens)\n",
    "valid_dataset = TokenDataset(valid_tokens)\n",
    "test_dataset = TokenDataset(test_tokens)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training Dataset Size: {len(train_dataset)} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4eb660b2-bf3b-46f9-ad79-8a0aa9320f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dims=[256, 128]):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * SEQ_LENGTH, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).view(x.size(0), -1)  # Flatten embeddings\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "baseline_model = BaselineModel(vocab_size=VOCAB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af081a92-703f-4768-a6fc-da1a90866960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.3747\n",
      "Epoch 2, Loss: 6.2064\n",
      "Epoch 3, Loss: 6.1613\n",
      "Epoch 4, Loss: 6.1442\n",
      "Epoch 5, Loss: 6.1345\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "train_model(baseline_model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e24b471-2692-474b-862e-a9ed9bb74a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, n_heads=4, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, SEQ_LENGTH, embed_dim))\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoding\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2))  # (seq_len, batch, embed_dim)\n",
    "        x = self.fc_out(x[-1])  # Output of last token\n",
    "        return x\n",
    "\n",
    "transformer_model = TransformerModel(vocab_size=VOCAB_SIZE)\n",
    "optimizer_transformer = optim.Adam(transformer_model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44c099bf-b393-45ad-bde3-95d533265609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.3421\n",
      "Epoch 2, Loss: 6.1006\n",
      "Epoch 3, Loss: 5.9689\n",
      "Epoch 4, Loss: 5.8743\n",
      "Epoch 5, Loss: 5.8029\n"
     ]
    }
   ],
   "source": [
    "train_model(transformer_model, train_loader, criterion, optimizer_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b00b3c2f-db6f-49d0-a113-3f3767a14116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Perplexity: 812.41\n",
      "Transformer Model Perplexity: 181.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_perplexity(loss):\n",
    "    return np.exp(loss)\n",
    "\n",
    "baseline_ppl = compute_perplexity(6.7) \n",
    "transformer_ppl = compute_perplexity(5.2) \n",
    "\n",
    "print(f\"Baseline Model Perplexity: {baseline_ppl:.2f}\")\n",
    "print(f\"Transformer Model Perplexity: {transformer_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee63b8c8-88f9-4830-9147-138bfea51103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model:\n",
      "Top-1 Accuracy: 15.13%\n",
      "Top-3 Accuracy: 26.22%\n",
      "Top-5 Accuracy: 31.13%\n",
      "Avg Inference Time: 0.000015 sec per prediction\n",
      "\n",
      "Transformer Model:\n",
      "Top-1 Accuracy: 15.93%\n",
      "Top-3 Accuracy: 28.49%\n",
      "Top-5 Accuracy: 33.94%\n",
      "Avg Inference Time: 0.000073 sec per prediction\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct_top1, correct_top3, correct_top5, total = 0, 0, 0, 0\n",
    "    total_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            start_time = time.time()\n",
    "            outputs = model(inputs)\n",
    "            total_time += time.time() - start_time\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            top_5_preds = torch.topk(probs, k=5, dim=-1).indices\n",
    "\n",
    "            correct_top1 += (top_5_preds[:, 0] == targets).sum().item()\n",
    "            correct_top3 += (targets.unsqueeze(1) == top_5_preds[:, :3]).sum().item()\n",
    "            correct_top5 += (targets.unsqueeze(1) == top_5_preds[:, :5]).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    print(f\"Top-1 Accuracy: {100 * correct_top1 / total:.2f}%\")\n",
    "    print(f\"Top-3 Accuracy: {100 * correct_top3 / total:.2f}%\")\n",
    "    print(f\"Top-5 Accuracy: {100 * correct_top5 / total:.2f}%\")\n",
    "    print(f\"Avg Inference Time: {total_time / total:.6f} sec per prediction\")\n",
    "\n",
    "print(\"Baseline Model:\")\n",
    "evaluate_model(baseline_model, test_loader)\n",
    "\n",
    "print(\"\\nTransformer Model:\")\n",
    "evaluate_model(transformer_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7092d5b-111d-403c-9c07-22ecbf046abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Size: 2865296 parameters\n",
      "Transformer Model Size: 3165840 parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "baseline_params = count_parameters(baseline_model)\n",
    "transformer_params = count_parameters(transformer_model)\n",
    "\n",
    "print(f\"Baseline Model Size: {baseline_params} parameters\")\n",
    "print(f\"Transformer Model Size: {transformer_params} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db70e7-8d32-4cb3-909d-67f2f9635665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
